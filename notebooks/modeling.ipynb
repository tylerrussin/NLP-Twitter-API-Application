{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter API connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "TWITTER_CONSUMER_API_KEY = getenv('TWITTER_CONSUMER_API_KEY')\n",
    "TWITTER_CONSUMER_API_SECRET = getenv('TWITTER_CONSUMER_API_SECRET')\n",
    "TWITTER_ACCESS_TOKEN = getenv('TWITTER_ACCESS_TOKEN')\n",
    "TWITTER_ACCESS_TOKEN_SECRET = getenv('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "TWITTER_AUTH = tweepy.OAuthHandler(TWITTER_CONSUMER_API_KEY,TWITTER_CONSUMER_API_SECRET)\n",
    "TWITTER_AUTH.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
    "TWITTER = tweepy.API(TWITTER_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''Cleaning raw tweet for modeling'''\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "    emoji_list = tweet.split()                  # Creating list to reference emojis\n",
    "    tweet = re.sub('[^a-z 0-9]', '', tweet)\n",
    "    tweet = tweet.split(' ')\n",
    "    output_string = ''\n",
    "\n",
    "    # Creating output string, handeling links and emojis\n",
    "    for index, token in enumerate(tweet):\n",
    "        if token[0:4] == 'http':\n",
    "            pass\n",
    "        elif token == '':\n",
    "            try:\n",
    "                output_string = output_string + ' ' + str(ord(emoji_list[index]))   # Replacing emoji with number value\n",
    "            except:\n",
    "                # Token is not an emoji\n",
    "                pass\n",
    "        else:\n",
    "            output_string = output_string + ' ' + token\n",
    "\n",
    "    output_string = output_string[1:]\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect tweets and generate dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_tweets(user1, user2):\n",
    "    tweets = []\n",
    "    users = []\n",
    "\n",
    "    for status in tweepy.Cursor(TWITTER.user_timeline, screen_name=user1, tweet_mode=\"extended\", count=200).items():\n",
    "        tweets.append(clean_tweet(status.full_text))\n",
    "        users.append(user1)\n",
    "\n",
    "    for status in tweepy.Cursor(TWITTER.user_timeline, screen_name=user2, tweet_mode=\"extended\", count=200).items():\n",
    "        tweets.append(clean_tweet(status.full_text))\n",
    "        users.append(user2)\n",
    "\n",
    "    return pd.DataFrame({'tweet':tweets, 'user':users})\n",
    "\n",
    "user1 = '@nasa'\n",
    "user2 = '@justinbieber'\n",
    "df = generate_tweets(user1, user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split and vectorize data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['user']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_vector = vectorizer.fit_transform(X_train)\n",
    "test_vector = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preform logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9637577491654745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clr = LogisticRegression()\n",
    "clr.fit(train_vector, y_train.values.ravel())\n",
    "scores = clr.score(test_vector, y_test) # accuracy\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@nasa'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'This week at NASA, we celebrate @NASAPersevere'\n",
    "tweet = clean_tweet(tweet)\n",
    "tweet = pd.DataFrame({'tweet':[tweet]})\n",
    "tweet = vectorizer.transform(tweet['tweet'])\n",
    "clr.predict(tweet)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b8e1b44fef6f769bdcd84daa9e6bcf329c54142ae85aa21700236386f9708da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
